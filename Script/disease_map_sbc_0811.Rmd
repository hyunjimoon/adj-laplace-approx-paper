---
title: "disease_map_sbc"
output:
  html_document: default
  pdf_document: default
---

```{r echo=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(
  include = TRUE,  cache = FALSE,  collapse = TRUE,  echo = TRUE,
  message = FALSE, tidy = FALSE,  warning = FALSE,   comment = "  ",
  dev = "png", dev.args = list(bg = '#FFFFF8'), dpi = 300,
  fig.align = "center",  fig.width = 7,  fig.asp = 0.618,  fig.show = "hold",
  out.width = "90%")
```


```{r}
library(ggplot2);
library(knitr); 
library(tidyverse);
library(rstan);
library(tufte);
library(parallel);
library(cmdstanr);
library(posterior);
library("bayesplot");
library("ggplot2");
library("rstanarm");
set_cmdstan_path("/Users/hyunjimoon/Dropbox/20_paper/charles/code/cmdstan")
options(digits = 2);  options(htmltools.dir.version = FALSE)
parallel:::setDefaultClusterOptions(setup_strategy = "sequential")
modelName <- "disease_map_ela_sbc_n"
dataFile <- "disease_data_100.r"
scriptDir <- getwd()
modelDir <- file.path(scriptDir, "models")
dataDir <- file.path(scriptDir, "data")
delivDir <- file.path(scriptDir, "deliv", modelName)
nChains <- 4
parallel_chains <- min(nChains, detectCores())

data <- read_rdump(file.path(dataDir, dataFile))
println <- function(msg) cat(msg); cat("\n")
printf <- function(pattern, ...) println(sprintf(pattern, ...))
util <- new.env()
source('stan_utility.R', local=util)
source('gp_utility.R', local=util)
c_light <- c("#DCBCBC")
c_light_highlight <- c("#C79999")
c_mid <- c("#B97C7C")
c_mid_highlight <- c("#A25050")
c_dark <- c("#8F2727")
c_dark_highlight <- c("#7C0000")

c_light_trans <- c("#DCBCBC80")
c_light_highlight_trans <- c("#C7999980")
c_mid_trans <- c("#B97C7C80")
c_mid_highlight_trans <- c("#A2505080")
c_dark_trans <- c("#8F272780")
c_dark_highlight_trans <- c("#7C000080")

c_light_teal="#6B8E8E"
c_mid_teal="#487575"
c_dark_teal="#1D4F4F"

c_green_trans <- c("#00FF0080")
c_superfine <- c("#8F272705")
```

SBC is one of the few tools that could examine choice of computational method (Talts, 2018). 
When SBC returns nonunifrom results, it does not pinpoint the cause of the problem; only the fact that something is wrong. Therefore, listing the problem casuing candidate could assist the job of upgrading the model that could pass the uniformity test in the end. This list differs according to the purpose of SBC.

SBC could largely applied for the following two purposes: 1. self-diagnose a model 2. diagnose a validity of approximate model. For the first purpose, identical models are used to generate data on which the model is fit to retrieve the posterior draws.On the otherhand, for the second purpose, while standard model is used to generate data, we apply approximate models to fit the data. In this case study, standard poission model generates with std model and fits with ela model.

It is worth noting that prior as well as likelihood is being tested in SBC. There are many reasons why prior cannot be given in advance especially for SBC.
1. Data-dependent: generally prior is set so that 10% of both sides of tails are cut. For example, in Talts(2018), for INLA sbc in gaussian process, ρ and σ is set so that Pr( ρ < 0.1) = Pr(σ > 1) = Pr(τ > 1) = 0.1 is met. Therefore, different priors are used depending on the characteristics of data.

2. SBC is more prior-sensitive.
In real world data-fitting situation, the existence of data(likelihood) offsets the effect of mis-specified prior. But in sbc situation, the effect of prior is greater as data itself is generated from the prior. When prior is too-heavy tailed, for example, it simulates unreasonable values which the model might struggle to fit.

Based on this note, at the end of SBC, conclusions like "this model works well with the _normal_ prior whose mean and standard deviation is _in range of 0 and 1_" is possible. Similary, but with some change, conclusions such as "this approximation model works well for _Poisson likelihood_ and _normal_ prior whose mean and standard deviation is _in range of 0 and 1_" is possible. In our setting the prior is given on marginal standard deviation($\alpha$) and length scale parameter($\rho$).

Here comes the list: prior, likelihood, computational method are what should be checked upon once you encounter nonuniformity in SBC plot. For approximation validity check purpose, the computaional mehtod. For the prior, one could change the prior model, from half-normal to inv-gamma for example, its parameter values. Likelihood can be modified similarly. The compuational method refers to the specific mechanism by which the samples are retrieved given the prior and likelihood. This cause has a large scope; MCMC(Rhat), HMC(), approximate models (adjoint-differential embedded laplace model, ADVI, INLA). A few Compuationa method can have its own diagnositcs and by comparing the diagnostic and eliminating out the candidates. As no single diagnostic method can tell all problems, It is important to interpret the results through a multiple testing lens.

Based on this understanding of SBC, each three types of sources are to be tested and compared.


```{r sbc, echo=FALSE}
sbcFitFile <- function(save_progress, stanmodel, modelName, S) {
  file.path(save_progress, paste0(modelName, '-', S, '.rda'))
}

sbc <- function(stanmodel, modelName, data, N, M, n_eff_reltol=0.2, ..., save_progress, load_incomplete=FALSE) {
  doSave <- !missing(save_progress)
  # parameter names
  stan_code <- stanmodel$code()
  stan_code <- scan(what = character(), sep = "\n", quiet = TRUE, text = stan_code)
  pars_lines <- grep("[[:space:]]*(pars_)|(pars_\\[.*\\])[[:space:]]*=", stan_code, value = TRUE)
  pars_lines <- pars_lines[!grepl("^[[:space:]]*vector", pars_lines) & 
                             !grepl("^[[:space:]]*real", pars_lines)]   
  pars_names <- trimws(sapply(strsplit(pars_lines, split = "=", fixed = TRUE), tail, n = 1))
  pars_names <- unique(sub("^([a-z,A-Z,0-9,_]*)_.*;", "\\1", pars_names))
  noUnderscore <- grepl(";", pars_names, fixed=TRUE)
  
  if (!load_incomplete) {
    todo <- seq(1:N)
  } else {
    mn <- modelName
    runs <- dir(save_progress)
    runs <- runs[grepl(paste0("^", mn,'-(\\d+).rda$'), runs)]
    if (length(runs) == 0) {
      stop(paste("No completed runs found in", dir,
                 "matching regular expression", paste0("^", mn,'-(\\d+).rda$'),
                 "\nDid you use sbc(..., save_progress='/path/to/results')?"))
    }
    todo <- as.integer(sub(paste0(mn,'-(\\d+).rda'), "\\1", runs))
  }
    post = list()      
  for(n in 1:N) {     
    S <- seq(1:N)[n] #seq(from = 1, to = .Machine$integer.max, length.out = N)[n]
    #load if exists
    if (doSave) {
      
      file <- sbcFitFile(save_progress, stanmodel,modelName, S) #TODO data chage should be reflected in fitname
      if (file.exists(file)) {
        got <- try(load(file), silent=TRUE)
        print(got)
        print(S)
        print(out)
        post[[n]] <- out
        next
      }
    }
    #iter_sampling = M = 3 * (20n - 1) # iter_warmup = 500, 597,
    out <- stanmodel$sample(data, chains = 1, iter_warmup = 500, iter_sampling = 597,   parallel_chains = 1, adapt_delta = 0.8,  save_warmup = FALSE, thin = 1) 
    #save
    if (doSave) {
      save(out, file=file)
    }
  }

  # prior predictive distribution
  Y <- sapply(post, FUN = function(p) {
    summary <- p$summary()
    summary %>%
      filter(str_detect(variable, "y_$|y_\\[.*\\]")) %>%
      pull(mean)
  })
  
  # realizations of parameter
  pars <- sapply(post, FUN = function(p) {
    summary <- p$summary()
    summary %>%
      filter(str_detect(variable, "pars_\\[[[:digit:]]+\\]")) %>%
      pull(mean)
  })
  
  # ranks: unthinned binary values for draw > true
  ranks <- lapply(post, FUN = function(p) {
    r <- subset_draws(p$draws(), variable = "ranks_") %>%
      as_draws_matrix()
    if (is.null(dim(r))) {
      r <- as.matrix(r)
    }
    print(r)
    print(pars_names)
    colnames(r) <- pars_names
    r[] <- r > 0
    return(r)
  })
  
  # divergences
  # sampler_params <- lapply(post, FUN = function(p){ p$sampler_diagnostics() %>%
  #     as_draws_matrix()
  # })
  out <- list(ranks = ranks, Y = Y, pars = pars, fits = post)#TODO sampler_params, ranks_ theta  #, sampler_params = sampler_params) 
  return(out)
}

ppc.sbc <- function(x, thin = 3, ...){
  thinner <- seq(from = 1, to = dim(x$Y)[1], by = thin)
  yhat <- x$Y[thinner,]
  yhat_df<- data.frame(matrix(yhat, ncol = N))
  yhat_df$idu <- as.numeric(row.names(yhat_df))
  yhat_df.melt <- reshape2::melt(yhat_df, id.vars="idu")
  ggplot(yhat_df.melt, aes(idu, value, col=variable)) + 
    geom_line()
}

plot.sbc <- function(x, thin = 3, ...) {
  thinner <- seq(from = 1, to = nrow(x$ranks[[1]]), by = thin)
  u <- t(sapply(x$ranks, FUN = function(r) 1L + colSums(r[thinner, , drop = FALSE])))
  parameter <- as.factor(rep(colnames(u), each = nrow(u)))
  d <- data.frame(u = c(u), parameter)
  bins <- 20
  binwidth <- (length(thinner) + 1) / bins
  suppressWarnings(ggplot2::ggplot(d, aes(x = u)) +
                      geom_histogram(binwidth = binwidth, color = "black",
                      fill = "#ffffe8", boundary = 0) +
                      facet_wrap(vars(parameter)) +
                      theme(panel.spacing.x = unit(2, "lines"))) +
                      geom_hline(yintercept= nrow(u)/bins)
}

uniformity.sbc <- function(x, bins = 20, thin = 3){
  samples <- nrow(x$ranks[[1]])
  thinner <- seq(from = 1, to = samples, by = thin)
  ranks <- t(sapply(x$ranks, FUN = function(r) 1L + colSums(r[thinner, , drop = FALSE])))
  parameter <- as.factor(rep(colnames(ranks), each = nrow(ranks)))
  num_params <- length(colnames(ranks))
  M = samples / thin
  max_rank = M + 1
  bin_size <- max_rank / bins
  pval <- rep(NA, num_params)

  for (i in 1:num_params){
    bin_count <- rep(0, bins)
    for (m in 1: length(ranks[,1])) {
      bin <- ceiling(ranks[m,i] / bin_size)
      bin_count[bin] <- bin_count[bin] + 1
    }
  # sum of bin_count is N (= total fit number)
  pval[i] <- chisq.test(bin_count)$p.value
  print("bin_count")
  print(bin_count)
  print("pval")
  print(pval)
  }
  #list(bin_count, pval)
}
```
1. thinnig: all posterior samples (= thin * M = 3 * 199) are stored and thinned before plotting and uniformity test.

2. prior predictive check(@not needed in our context): In general, prior predictive checking is necessary as sbc assess the accuracy of computational method only within the context that prior samples are generated through real date generating proceess; on which the posterior distributions are learned. Prior predictive checking is one way to do this.However, when the purpose is to measure how well approximate model is, the original model is assumed as the true data generating model. Therefore, comparing the generated prior samples with the original dataset is not necessary. 

## 1. prior comparison
prior is not fixed for the model for two reasons.1. Data-dependentgenerally prior is set so that 10% of both sides of tails are cut. For example, Pr( ρ < 0.1) = Pr(σ > 1) = Pr(τ > 1) = 0.1.2. SBC is more prior-sensitive. In real world data-fitting situation, the existence of data(likelihood) offsets the effect of mis-specified prior. But in sbc situation, the effect of prior is greater as data itself is generated from the prior.


Let's plot sbc plot for each parameter varying the mean and the variance for the two hyperparameters. When 0 is used for the means, divergences were observed and the numerous fit were never completed. 
```{r}
# modelDir <- file.path(scriptDir, "models")
# dataDir <- file.path(scriptDir, "data")
# delivDir <- file.path(scriptDir, "deliv", modelName)
# file <- file.path(modelDir, modelName, paste0(modelName, ".stan"))
# mod <- cmdstan_model(file, quiet = FALSE)
# submodelName <- "disease_map_ela_sbc_n_m0_sddot1"
# data$alpha_mu_prior <- 0
# data$alpha_sd_prior <- 0.1
# data$rho_mu_prior <- 0
# data$rho_sd_prior <- 0.1
# 
# N = 20 #(bins = 20)*N # num of sim and fit
# M = 597 # 897 (6n -3) # num of post. draw - related to iter_sampling - not used currently
# sbc_res_m0_sddot1<- sbc(mod, submodelName, data = data, N = N, M = M, save_progress = delivDir)
# ppc.sbc(sbc_res_m0_sddot1)
# uniformity.sbc(sbc_res_m0_sddot1)
# plot.sbc(sbc_res_m0_sddot1)                                                          
```
 
Posterior mean were learned and plugged in; 0.7 and 2.2. Next we try to vary the sd starting from 0.01 until the point when it breaks.

```{r}
modelDir <- file.path(scriptDir, "models")
dataDir <- file.path(scriptDir, "data")
delivDir <- file.path(scriptDir, "deliv", modelName)
file <- file.path(modelDir, modelName, paste0(modelName, ".stan"))
mod <- cmdstan_model(file, quiet = FALSE)
submodelName <- "disease_map_ela_sbc_n_dot01"
data$alpha_mu_prior <- 0.7
data$alpha_sd_prior <- 0.01
data$rho_mu_prior <- 2.2
data$rho_sd_prior <- 0.01

N = 100 #(bins = 20)*N # num of sim and fit
M = 597 # 897 (6n -3) # num of post. draw - related to iter_sampling - not used currently
sbc_res_me_dot01<- sbc(mod, submodelName, data = data, N = N, M = M, save_progress = delivDir)
ppc.sbc(sbc_res_me_dot01)
uniformity.sbc(sbc_res_me_dot01)
plot.sbc(sbc_res_me_dot01)                                                          
```

```{r}
submodelName <- "disease_map_ela_sbc_n_dot05"
data$alpha_mu_prior <- 0.7
data$alpha_sd_prior <- 0.05
data$rho_mu_prior <- 2.2
data$rho_sd_prior <- 0.05
N = 100 #(bins = 20)*N # num of sim and fit
M = 597 # 897 (6n -3) # num of post. draw - related to iter_sampling - not used currently
sbc_res_me_sddot05<- sbc(mod, submodelName, data = data, N = N, M = M, save_progress = delivDir)
ppc.sbc(sbc_res_me_sddot05)
uniformity.sbc(sbc_res_me_sddot05)
plot.sbc(sbc_res_me_sddot05)
```

```{r}
submodelName <- "disease_map_ela_sbc_n_dot1"
data$alpha_mu_prior <- 0.7
data$alpha_sd_prior <- 0.1
data$rho_mu_prior <- 2.2
data$rho_sd_prior <- 0.1
N = 100 #(bins = 20)*N # num of sim and fit
M = 597 # 897 (6n -3) # num of post. draw - related to iter_sampling - not used currently
sbc_res_me_sddot1<- sbc(mod, submodelName, data = data, N = N, M = M, save_progress = delivDir)
ppc.sbc(sbc_res_me_sddot1)
uniformity.sbc(sbc_res_me_sddot1)
plot.sbc(sbc_res_me_sddot1)
```


```{r}
submodelName <- "disease_map_ela_sbc_n_1"
data$alpha_mu_prior <- 0.7
data$alpha_sd_prior <- 1
data$rho_mu_prior <- 2.2
data$rho_sd_prior <- 1
N = 100 #(bins = 20)*N # num of sim and fit
M = 597 # 897 (6n -3) # num of post. draw - related to iter_sampling - not used currently
sbc_res_me_sd1<- sbc(mod, submodelName, data = data, N = N, M = M, save_progress = delivDir)
ppc.sbc(sbc_res_me_sd1)
uniformity.sbc(sbc_res_me_sd1)
plot.sbc(sbc_res_me_sd1)
```


## 2. likelihood comparison
It is known that bernouli likelihood, log-concave though it is, is not friendly to approximation compared to normal, Poisson, binomial, and negative binomial likelihood. The approximation introduces a bias(Kuss and Rasmussen (2005), Vanhatalo, Pietiläinen, and Vehtari (2010), Cseke and Heskes (2011), Vehtari et al. (2016)). Therefore, the sbc of poisson likelihood shows greater uniformity than bernoulli.


## 3. compuational method comparison
advi(or inla) both of which are known to indrouce bias is compared with ela inference. 
The alternative to SBC could be [PSIS](https://mc-stan.org/loo/reference/pareto-k-diagnostic.html) that is performed based on log_lik in generated quantities block. PSIS diagnostics might be computationally cheaper than SBC, but is a diagnostics that concentrates on specific data rather than model itself. Moreover, PSIS cannot assess the faithfulness of our computational methods.

If the rank outcome could be expressed as the function of posterior $E_{post}[I(\alpha <\alpha_{sim})]$ then, sensitivity of 

#model naming
empirical mean: no m, yes sd /e.g. disease_map_ela_sbc_n_dot1
0 mean: yes m, yes sd /e.g. disease_map_ela_sbc_n_m0_sddot1
<!-- for (i in 1:N){ -->
<!--   tmp <- sbc_res2$fits[[i]]$output_files() -->
<!--   rstanfit_sbc <- read_stan_csv(tmp) -->
<!--   util$check_all_diagnostics(rstanfit_sbc) -->
<!--   partition <- util$partition_div(rstanfit_sbc) -->
<!--   div_samples <- partition[[1]] -->
<!--   nondiv_samples <- partition[[2]] -->

<!--   par(mfrow=c(1, 3)) -->
<!--   plot(nondiv_samples$rho, nondiv_samples$alpha, log="xy", -->
<!--        col=c_dark_trans, pch=16, cex=0.8, xlab="rho", ylab="alpha") -->
<!--   points(div_samples$rho, div_samples$alpha, -->
<!--          col=c_green_trans, pch=16, cex=0.8) -->
<!-- } -->
<!-- theta도 해보는게 좋을까? alpha, rho (hp) 가 theta와 높은상관성 -->
<!-- plot(nondiv_samples$rho, nondiv_samples$sigma_error_ship, -->
<!--      col=c_dark_trans, pch=16, cex=0.8, xlab="length_GP_engine", ylab="sigma_error_ship") -->
<!-- points(div_samples$length_GP_engine, div_samples$sigma_error_ship, -->
<!--        col=c_green_trans, pch=16, cex=0.8) -->

<!-- plot(nondiv_samples$sigma_GP_engine, nondiv_samples$sigma_error_ship, -->
<!--      col=c_dark_trans, pch=16, cex=0.8, xlab="sigma_GP_engine", ylab="sigma_error_ship") -->
<!-- points(div_samples$sigma_GP_engine, div_samples$sigma_error_ship, -->
<!--        col=c_green_trans, pch=16, cex=0.8) -->
